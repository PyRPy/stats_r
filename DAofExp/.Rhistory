lda.fit <- lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
lda.fit
plot(lda.fit)
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1]>.9)
qda.fit <- qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
qda.class <- predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
library(class)
train.X <- cbind(Lag1,Lag2)[train,]
test.X <- cbind(Lag1,Lag2)[!train,]
train.Direction <- Direction[train]
set.seed(1)
knn.pred <- knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
(83+43)/252
knn.pred <- knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
dim(Caravan)
detach(Smarket)
dim(Caravan)
# An Application to Caravan Insurance Data
head(Caravan)
attach(Caravan)
summary(Purchase)
summary(Purchase)
glimpse(Caravan)
# Scale the data sets
standardized.X <- scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test <- 1:1000
train.X <- standardized.X[-test,]
test.X <- standardized.X[test,]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
table(knn.pred,test.Y)
9/(68+9)
knn.pred <- knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
5/26
knn.pred <- knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
glm.fits <- glm(Purchase~.,data=Caravan,family=binomial,subset=-test)
glm.probs <- predict(glm.fits,Caravan[test,],type="response")
glm.pred <- rep("No",1000)
glm.pred[glm.probs>.5] <- "Yes"
table(glm.pred,test.Y)
glm.pred=rep("No",1000)
glm.pred<- rep("No",1000)
glm.pred[glm.probs>.25] <- "Yes"
table(glm.pred,test.Y)
11/(22+11)
# iris data set classification by 'keras'
library(keras)
library(caret)
head(iris)
# Prepare 'training' and 'testing' sets
index<-createDataPartition(iris$Species,p=0.7,list=F)
Train_Features <- data.matrix(iris[index,-5])
Train_Labels <- iris[index,5]
Test_Features <- data.matrix(iris[-index,-5])
Test_Labels <- iris[-index,5]
createDataPartition()
createDataPartition
# Prepare 'training' and 'testing' sets
index<-createDataPartition(iris$Species,p=0.7,list=TRUE)
Train_Features <- data.matrix(iris[index,-5])
# Prepare 'training' and 'testing' sets
index<-unlist(createDataPartition(iris$Species,p=0.7,list=TRUE))
Train_Features <- data.matrix(iris[index,-5])
Train_Labels <- iris[index,5]
Test_Features <- data.matrix(iris[-index,-5])
Test_Labels <- iris[-index,5]
index
length(index)
to_categorical(as.numeric(Train_Labels))[,c(-1)] -> Train_Labels
to_categorical(as.numeric(Test_Labels))[,c(-1)] -> Test_Labels
summary(Train_Labels)
str(Train_Features)
# Prepare the model
model <- keras_model_sequential()
model %>%
layer_dense(units=10,activation = "relu",input_shape = ncol(Train_Features)) %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>% compile(loss = "categorical_crossentropy",
optimizer = optimizer_adagrad(),
metrics = c('accuracy')
)
# run the model
history <- model %>% fit(Train_Features,Train_Labels,
validation_split = 0.10,
epochs=300,
batch_size = 5,
shuffle = T)
# plot the history
plot(history)
# model evaluation
model %>% evaluate(Test_Features,Test_Labels)
# check with logistic regression results
model.logit <- glm(Species ~
Petal.Width+
Petal.Length+
Sepal.Width+
Sepal.Length,
data=iris[index, ],
family = binomial(logit))
predict(model.logit, iris[-index, ])
# iris classifcation by 'keras'
library(keras)
head(iris)
index <- sample(1:nrow(iris), size = round(0.7*n), replace=FALSE)
data_train <- iris[index,]
data_test <- iris[-index, ]
y <- data_train[, "Species"]
x <- data_train[,1:4]
# scale to [0,1]
x <- as.matrix(apply(x, 2, function(x) (x-min(x))/(max(x) - min(x))))
levels(y) <- 1:length(y)
y <- to_categorical(as.integer(y) - 1 , num_classes = 3)
y
# create sequential model
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
fit <- model %>%
fit(
x = x,
y = y,
shuffle = T,
batch_size = 5,
validation_split = 0.3,
epochs = 200
)
plot(fit)
# model evaluation
model %>% evaluate(data.matrix(data_test[, 1:4]), data_test[, 5])
# model evaluation
model %>% evaluate(data.matrix(data_test[, 1:4]), data.matrix(data_test[, 5])
# model evaluation
model %>% evaluate(data.matrix(data_test[, 1:4]), data.matrix(data_test[, 5]))
# index <- sample(1:nrow(iris), size = round(0.7*n), replace=FALSE)
index <- 1: nrow(iris)
data_train <- iris[index,]
data_test <- iris[-index, ]
y <- data_train[, "Species"]
x <- data_train[,1:4]
# scale to [0,1]
x <- as.matrix(apply(x, 2, function(x) (x-min(x))/(max(x) - min(x))))
levels(y) <- 1:length(y)
y <- to_categorical(as.integer(y) - 1 , num_classes = 3)
y
# create sequential model
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
fit <- model %>%
fit(
x = x,
y = y,
shuffle = T,
batch_size = 5,
validation_split = 0.3,
epochs = 200
)
plot(fit)
# index <- sample(1:nrow(iris), size = round(0.7*n), replace=FALSE)
index <- sample(nrow(iris))
data_train <- iris[index,]
data_test <- iris[-index, ]
y <- data_train[, "Species"]
x <- data_train[,1:4]
# scale to [0,1]
x <- as.matrix(apply(x, 2, function(x) (x-min(x))/(max(x) - min(x))))
levels(y) <- 1:length(y)
y <- to_categorical(as.integer(y) - 1 , num_classes = 3)
y
# create sequential model
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
fit <- model %>%
fit(
x = x,
y = y,
shuffle = T,
batch_size = 5,
validation_split = 0.3,
epochs = 200
)
# add dropout to reduce overfitting
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units = 3, activation = "softmax")
model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
fit = model %>%
fit(
x = x,
y = y,
shuffle = T,
validation_split = 0.3,
epochs = 200,
batch_size = 5
)
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
lm.fit <- lm(medv~lstat)
attach(Boston)
lm.fit <- lm(medv~lstat)
lm.fit <- lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit <- lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
install.packages("lme4")
library(lme4)
# read data
data(dietox, package = 'geepack')
install.packages("geepack")
# read data
data(dietox, package = 'geepack')
# inspect data
head(dietox)
print(summary(lmer('Weight ~ Time + (1|Pig'), data=dietox)))
print(summary(lmer('Weight ~ Time + (1|Pig)', data=dietox)))
print(summary(lmer("Weight ~ Time + (1 + Time | Pig)", data=dietox)))
fm0 <- lm(Weight ~ Time + Cu + Cu*Time, data = deitox)
fm0 <- lm(Weight ~ Time + Cu + Cu*Time, data = dietox)
par(mfrow = c(2, 3))
install.packages("doBy")
# http://www.math.ttu.edu/~atrindad/software/MixedModels-RandSAScomparison.pdf
dietox$Cu <- as.factor(dietox$Cu)
plotBy(Weight ~ Time, subject = Pig, group = Cu, title = "Cu=",
data = dietox, col = 1:100, lines = T)
library(doBy)
plotBy(Weight ~ Time, subject = Pig, group = Cu, title = "Cu=",
data = dietox, col = 1:100, lines = T)
plotBy
?plotBy
plotby(Weight ~ Time, subject = Pig, group = Cu, title = "Cu=",
data = dietox, col = 1:100, lines = T)
m.dietox <- summaryBy(Weight ~ Cu + Time, data = dietox,
FUN = mean)
m.dietox[1:5, ]
summary(fm0)
# adding a random intercept term
fm1 <- lme(Weight ~ Time + Cu + Cu * Time, data = dietox,
random = ~1 | Pig)
library(lme4)
# adding a random intercept term
fm1 <- lme(Weight ~ Time + Cu + Cu * Time, data = dietox,
random = ~1 | Pig)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + Cu * Time, data = dietox,
random = ~1 | Pig)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + Cu * Time + (1|Pig) + (1|Evit), data = dietox)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + (1|Pig) + (1|Evit), data = dietox)
summary(fm1)
library(geepack)
data(dietox)
dietox$Cu <- as.factor(dietox$Cu)
l1 <- lme4::lmer(Weight ~ Time + Cu + (1|Pig) + (1|Evit), data = dietox)
lmer.display(l1)
summary(l1)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + (1|Pig), data = dietox)
summary(fm1)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + Cu*Time + (1|Pig), data = dietox)
summary(fm1)
# add a random slope term
fm2 <- lme(Weight ~ Cu * Time + (1|Time/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Cu * Time + (1|Time/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Cu * Time + (1|Time|Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Time/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Cu + (1|Time/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ (1|Time/Pig), data = dietox)
# adding a random intercept term
fm1 <- lmer(Weight ~ Time + Cu + Cu*Time + (1|Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Pig ), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Cu ), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Cu), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Pig/Evit), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Evit), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Evit/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + Cu + (1|Cu), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + (1|Cu), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + (1|Cu/Pig), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + (1|Cu/Litter), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + (1|Litter), data = dietox)
# add a random slope term
fm2 <- lmer(Weight ~ Time + (1|Litter/Cu), data = dietox)
setwd("C:/Users/K Li/OneDrive/Documents/Data Science/Book_Examples_R/DAoExp/R_code")
# Table 5.11, page 126
mung.data <- read.table("data/mungbean.txt", header=T)
# Table 5.11, page 126
mung.data <- read.table("Data/mungbean.txt", header=T)
# Table 5.11, page 126
mung.data <- read.table("Data/mungbean8.txt", header=T)
View(mung.data)
colnames(mung.data) <- c('Obs', 'Order', 'Trtmt', 'Length')
model1 <- aov(Length ~ factor(Trtmt), data=mung.data)
# Compute predicted values, residuals, standardized residuals, normal scores
mung.data <- within(mung.data, {
# Compute predicted, residual, and standardized residual values
ypred = fitted(model1); e = resid(model1); z = e/sd(e);
# Compute Blom's normal scores
n = length(e); q = rank(e); nscore = qnorm((q-0.375)/(n+0.25)) })
# Display first 3 lines of mung.data, 4 digits per variable
print(head(mung.data, 3), digits=4)
# Generate residual plots
plot(z ~ Trtmt, data=mung.data, ylab="Standardized Residuals", las=1)
# Generate residual plots
plot(z ~ Trtmt, data=mung.data, ylab="Standardized Residuals", las=1)
abline(h=0)  # Horizontal line at zero
plot(z ~ Order, data=mung.data, ylab="Standardized Residuals", las=1)
abline(h=0)
plot(z ~ ypred, data=mung.data, ylab="Standardized Residuals", las=1)
abline(h=0)
plot(z ~ nscore, data=mung.data, ylab="Standardized Residuals", las=1)
qqline(mung.data$z)  # Line through 1st and 3rd quantile points
# A simpler way to generate the normal probability plot
qqnorm(mung.data$z); qqline(mung.data$z)
# R code from text page 129
# NPPlot of standardized residuals by treatment
by(mung.data$z, mung.data$Trtmt, qqnorm)
# Or, as illustrated here for treatment 1
qqnorm(mung.data$z[mung.data$Trtmt == 1], main = "Normal Q-Q Plot: Trtmt = 1")
qqline(mung.data$z)  # Line through 1st and 3rd quantile points
# Table 6.15, page 185
react.data <- read.table("Data/reaction.time.txt", header=T)
react.data
View(react.data)
react.data <- head(react.data, 14) # Keep first 14 observations
head(react.data, 3)
# Create trtmt combo vbl TC and factors fTC, fA, and fB within data set
react.data <- within(react.data,
{TC = 10*A + B; fTC = factor(TC); fA = factor(A); fB = factor(B)})
summary(react.data[,c("fA","fB","fTC","y")])
head(react.data)
modelAB <- aov(y ~ fA + fB + fA:fB, data = react.data)
anova(modelAB) # Type I ANOVA
drop1(modelAB, ~., test = "F") # Type III ANOVA
modelTC <- aov(y ~ fTC, data = react.data)
anova(modelTC) # Model F-test
# Contrasts: estimates, CIs, tests
library(lsmeans)
# Main-effect-of-B contrast: B1-B2
lsmB <- lsmeans(modelAB, ~ fB)
summary(contrast(lsmB, list(B12=c( 1,-1, 0))), infer=c(T,T))
# AB-interaction contrast: AB11-AB13-AB21+AB23
lsmAB <- lsmeans(modelAB, ~ fB:fA) # Using "fB:fA" yields AB lex order
lsmAB # Display to see order of AB combos for contrast coefficients
summary(contrast(lsmAB, list(AB=c( 1 ,0,-1,-1, 0, 1))), infer=c(T,T))
# Multiple comparisons: B
confint(lsmB, level=0.99) # lsmeans for B and 99% CIs
# Tukey's method
summary(contrast(lsmB, method="pairwise", adjust="tukey"),
infer=c(T,T), level=0.95)
# Dunnett's method
summary(contrast(lsmB, method="trt.vs.ctrl", adj="mvt", ref=1),
infer=c(T,T), level=0.95)
summary(contrast(lsmTC, method="trt.vs.ctrl", adj="mvt"),
infer=c(T,T), level=0.95, side=">")
# Dunnett's method for all treatment versus control comparisons, text page 189
lsmTC <- lsmeans(modelTC, ~ fTC)
summary(contrast(lsmTC, method="trt.vs.ctrl", adj="mvt"),
infer=c(T,T), level=0.95, side=">")
air.data <- read.table("Data/air.velocity.contrasts.txt", header=T)
air.data
# Fit linear regression model, save as model1
model1 <- lm(y ~ Aln + Aqd + Bln + Bqd + Bcb + Bqr + Bqn
+ Aln:Bln + Aln:Bqd + Aln:Bcb + Aln:Bqr
+ Aqd:Bln + Aqd:Bqd + Aqd:Bcb, data=air.data)
# ANOVA
anova(model1)
setwd("~/GitHub/stats_r/DAofExp")
# Table 6.15, page 185
react.data <- read.table("Data/reaction.time.txt", header=T)
react.data
react.data <- head(react.data, 14) # Keep first 14 observations
head(react.data, 3)
# Create trtmt combo vbl TC and factors fTC, fA, and fB within data set
react.data <- within(react.data,
{TC = 10*A + B; fTC = factor(TC); fA = factor(A); fB = factor(B)})
head(react.data)
summary(react.data[,c("fA","fB","fTC","y")])
# ANOVA
options(contrasts = c("contr.sum", "contr.poly"))
modelAB <- aov(y ~ fA + fB + fA:fB, data = react.data)
anova(modelAB) # Type I ANOVA
drop1(modelAB, ~., test = "F") # Type III ANOVA
modelTC <- aov(y ~ fTC, data = react.data)
anova(modelTC) # Model F-test
# Contrasts: estimates, CIs, tests
library(lsmeans)
# Main-effect-of-B contrast: B1-B2
lsmB <- lsmeans(modelAB, ~ fB)
summary(contrast(lsmB, list(B12=c( 1,-1, 0))), infer=c(T,T))
# AB-interaction contrast: AB11-AB13-AB21+AB23
lsmAB <- lsmeans(modelAB, ~ fB:fA) # Using "fB:fA" yields AB lex order
lsmAB # Display to see order of AB combos for contrast coefficients
summary(contrast(lsmAB, list(AB=c( 1 ,0,-1,-1, 0, 1))), infer=c(T,T))
# Multiple comparisons: B
confint(lsmB, level=0.95) # lsmeans for B and 99% CIs
# Tukey's method
summary(contrast(lsmB, method="pairwise", adjust="tukey"),
infer=c(T,T), level=0.95)
# Dunnett's method
summary(contrast(lsmB, method="trt.vs.ctrl", adj="mvt", ref=1),
infer=c(T,T), level=0.95)
# Dunnett's method for all treatment versus control comparisons, text page 189
lsmTC <- lsmeans(modelTC, ~ fTC)
summary(contrast(lsmTC, method="trt.vs.ctrl", adj="mvt"),
infer=c(T,T), level=0.95, side=">")
plot(y ~ A, data=react.data, xaxt="n", type="n") # Suppress x-axis, pts
axis(1, at=seq(1,2,1)) # Add x-axis with tick marks from 1 to 2 by 1
text(y ~ A, B, cex=0.75, data=react.data) # Plot z vs A using B label
mtext("B=1,2,3", side=3, adj=1, line=1) # Margin text, top-rt, line 1
abline(h=0) # Horizontal line at zero
interaction.plot(x.factor = react.data$fA, trace.factor = react.data$fB,
response = react.data$y, type ="b",
xlab ="A", trace.label ="B", ylab ="Mean of y")
plot(modelAB$res ~ react.data$TC, xlab ="AB", ylab ="Residual")
plot(react.data$y ~ react.data$TC, xlab ="AB", ylab ="y")
plot(modelAB$res ~ react.data$Trtmt, xaxt="n", xlab="AB", ylab="Residual")
axis(1, at = react.data$Trtmt, labels = react.data$fTC)
plot(react.data$y ~ react.data$Trtmt, xaxt="n", xlab="AB", ylab="y")
axis(1, at = react.data$Trtmt, labels = react.data$fTC)
by(modelAB$res, react.data$A, var)
